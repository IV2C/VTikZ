{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = \"\"\"\n",
    "You are an image classification agent. Your role is to evaluate whether a given instruction has been correctly applied to an image.\n",
    "You are given the original image, the modified image and an instruction.\n",
    " Response Format:\n",
    "1. Provide a step-by-step analysis of the image in relation to the instruction.  \n",
    "2. Conclude your response with either `<YES>` or `<NO>` on a new line, depending on whether the instruction was applied.  \n",
    "3. Ensure that `<YES>` or `<NO>` is enclosed within less than (`<`) and greater than (`>`) signs and appears on a separate line at the end of the response.  \n",
    "4. Ensure the less than (`<`) and greater than (`>`) signs are only used at the end of the response and nowhere else .\n",
    "\"\"\"\n",
    "PROMPT = \"\"\"\n",
    "Was the instruction \"{instruction}\" applied to the image?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # base_url=\"https://api.groq.com/openai/v1\", api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def assess_response(response: str) -> bool:\n",
    "    matches = re.search(r\"<(.{3})>\", response)\n",
    "    if not matches:\n",
    "        return \"<YES>\" in response\n",
    "    return matches.group(1) == \"YES\"\n",
    "\n",
    "\n",
    "def check_modification(\n",
    "    image_solution: Image.Image, instruction: str, image_input: Image.Image\n",
    ") -> bool:\n",
    "    buffered_input = BytesIO()\n",
    "    image_input.save(buffered_input, format=\"JPEG\")\n",
    "    img_str_input = base64.b64encode(buffered_input.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    buffered_solution = BytesIO()\n",
    "    image_solution.save(buffered_solution, format=\"JPEG\")\n",
    "    img_str_solution = base64.b64encode(buffered_solution.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        # model=\"llama-3.2-90b-vision-preview\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": PROMPT.format(instruction=instruction)\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_str_input}\",\n",
    "                            \"detail\": \"low\",\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_str_solution}\",\n",
    "                            \"detail\": \"low\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_completion_tokens=4096,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return assess_response(response), response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [05:06<00:00,  3.06s/ examples]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classification(row):\n",
    "    row[\"instruction_applied\"],row[\"response\"] = check_modification(row[\"image_solution\"][0],row[\"instruction\"],row[\"image_input\"])\n",
    "    return row\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"CharlyR/varbench\", \"tikz\", split=\"benchmark\")\n",
    "\n",
    "ds = ds.select_columns([\"id\",\"instruction\",\"image_solution\",\"image_input\"])\n",
    "\n",
    "ds = ds.map(classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6847.62 examples/s]t/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 166.57ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/CharlyR/vTikz-vlm_oracl_benchmark/commit/c18c623f552654b99226f18344850e71f48f23d9', commit_message='Upload dataset', commit_description='', oid='c18c623f552654b99226f18344850e71f48f23d9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/CharlyR/vTikz-vlm_oracl_benchmark', endpoint='https://huggingface.co', repo_type='dataset', repo_id='CharlyR/vTikz-vlm_oracl_benchmark'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"CharlyR/vTikz-vlm_oracl_benchmark\",\"input_provided_gpt4o-mini\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"instruction_applied\"] = df[\"instruction_applied\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.720000\n",
       "std        0.451261\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: instruction_applied, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"instruction_applied\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"instruction_applied\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
