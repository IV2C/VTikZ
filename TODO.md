# TODO

- [X] either make a script to add an entry in the dataset or just doucemnt the command to do it.
- [X] patch computation
- [X] Make the dataset with a python script
- [X] refactor dataset creation
- [X] change patch computation
- [X] refactor run-evaluation
  - [X] Use vllm and openai inference via a wrapper for evaluation(get inspired by https://github.com/HumanEval-V/HumanEval-V-Benchmark/blob/main/models/vllm_model.py)
- [X] Finish implementing openai batch api
- [X] Add an api option for run evaluation
- [X] Make a score object and change the evaluator to return it (change the tests as well)
- [X] Remake latex-renderer
- [X] Make some first simple data
- [X] text squid, find other possible solutions for dataset
- [X] implement pass@k
- [X] test pass@k
- [X] complex oracle
  - [X] vlm or computer-vision("Are the eyes red")
  - [X] Image comparison(image patch)
  - [X] line patch(need to think, either same as before with only one column patch, or a new one with only the lines)
- [X] Test renderer when image not compilable
- [X] Fix evaluator tests
- [X] fix line patch
- [X] Add vision input possibility in model class (both vllm and openai->https://platform.openai.com/docs/guides/vision)
- [X] fix workflow
- [X] Add a config file containing vllm config 
- [X] switch from vllm launch in python to openai compaptible server
- [X] Add local tests for each api types (class chat_api)
  - [X] launch all the test manually and then add a skip for all of them  
  - [X] add single request tests
  - [X] parameterize tests to test VLLMApi and GroqApi
  - [X] Add Parameter in VLMApi to start or not the api(get it from the config)
  - [X] change 30 to something bigger in sleep at line 201 in chat_api.py 
- [X] Finish testing the batch requests with openai
- [ ] Change Model class by agent, remove VLM/OPENAI parameter in run_generation and run_evaluation, instantiate api depending on the api_url provided, add in documentation that the api used depends on the url(and that VLLM is launched in the case of a VLLM api)(maybe add a parameter to prevent vllm from being launched or maybe the opposite)
- [ ] Add level of difficulty in the datasets
- [ ] Add backup at each step for api completions(need to think about how to implement it in the different types of apis, especially for openai batches)
- [ ] Check patches in the synthetic data generation
- [ ] Create synthetic data and then verify
- [ ] Make another submodel called multimodal that can take images, and allow for parametrization in the command line
- [ ] Make another submodel called multimodal-loop that can take images and loops until a satisfying image is created, and allow for parametrization in the command line.
- [ ] Assessment of vlm on checking 
- [ ] Test LLM-only models then compare with Multimodal with image in input
- [ ] Implement LLM+something solution and test it
- [ ] Explore which other things can be tested(tikz, svg, ascii, what else?)
Eventually
- [ ] ascii art "renderer"
- [X] github action
  - [ ] Tests
  - [ ] creates and publishes the dataset

